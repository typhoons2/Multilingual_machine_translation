{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7592022,"sourceType":"datasetVersion","datasetId":4419308},{"sourceId":7713356,"sourceType":"datasetVersion","datasetId":4503562},{"sourceId":7717109,"sourceType":"datasetVersion","datasetId":4507079},{"sourceId":7720853,"sourceType":"datasetVersion","datasetId":4415899},{"sourceId":7762915,"sourceType":"datasetVersion","datasetId":4537359},{"sourceId":7801296,"sourceType":"datasetVersion","datasetId":4430268},{"sourceId":7816582,"sourceType":"datasetVersion","datasetId":4436066},{"sourceId":7838579,"sourceType":"datasetVersion","datasetId":4443802},{"sourceId":7854731,"sourceType":"datasetVersion","datasetId":4424653},{"sourceId":7871117,"sourceType":"datasetVersion","datasetId":4468546}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers sentencepiece datasets sacrebleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-27T14:07:25.765320Z","iopub.execute_input":"2024-02-27T14:07:25.766093Z","iopub.status.idle":"2024-02-27T14:07:39.795461Z","shell.execute_reply.started":"2024-02-27T14:07:25.766044Z","shell.execute_reply":"2024-02-27T14:07:39.794534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom transformers import MT5Tokenizer\nfrom transformers import MT5Config, MT5ForConditionalGeneration\nfrom transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers import get_linear_schedule_with_warmup\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom tqdm import tqdm_notebook\nfrom tqdm import tqdm\nimport re\nimport sacrebleu\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:07:39.797920Z","iopub.execute_input":"2024-02-27T14:07:39.798313Z","iopub.status.idle":"2024-02-27T14:07:48.030137Z","shell.execute_reply.started":"2024-02-27T14:07:39.798274Z","shell.execute_reply":"2024-02-27T14:07:48.029117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regex_vietnamese = re.compile(r'[^a-zđáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựỳỵỷỹ\\s]', re.IGNORECASE)\nregex_english = re.compile(r\"[^a-zA-Z\\s']\", re.IGNORECASE)\nregex_spanish = re.compile(r'[^a-záéíóúñ\\s]', re.IGNORECASE)\n\ndef clean_text(text, regex):\n    if text and isinstance(text, str):\n        text = regex.sub('', text).lower().strip()\n        return \" \".join(text.split())\n    return \"\"\ndef load_and_prepare_data_vi_en(file_path):\n    df = pd.read_csv(file_path)\n    df['vi'] = df['vi'].apply(lambda x: clean_text(x, regex_vietnamese))\n    df['en'] = df['en'].apply(lambda x: clean_text(x, regex_english))\n\n    df.dropna(inplace=True)\n    df.drop_duplicates(inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    return df\n\n\ndef load_and_prepare_data_en_es(file_path):\n    df = pd.read_csv(file_path)\n    df['en'] = df['en'].apply(lambda x: clean_text(x, regex_english))\n    df['es'] = df['es'].apply(lambda x: clean_text(x, regex_spanish))\n\n    df.dropna(inplace=True)\n    df.drop_duplicates(inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    return df\n\ndef load_and_prepare_data_vi_es(file_path):\n    df = pd.read_csv(file_path)\n    df['vi'] = df['vi'].apply(lambda x: clean_text(x, regex_vietnamese))\n    df['es'] = df['es'].apply(lambda x: clean_text(x, regex_spanish))\n\n    df.dropna(inplace=True)\n    df.drop_duplicates(inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    return df\n\ndf_vi_en = load_and_prepare_data_vi_en('/kaggle/input/data-du-an-cong-nghe-thong-tin/vi-en/train_data_en_vi.csv')\ndf_en_es = load_and_prepare_data_en_es('/kaggle/input/data-du-an-cong-nghe-thong-tin/en_es/train_data_en_es.csv')\ndf_vi_es = load_and_prepare_data_vi_es('/kaggle/input/data-vi-es/df_vi_es.csv')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_vi_en = load_and_prepare_data_vi_en('/kaggle/input/data-du-an-cong-nghe-thong-tin/vi-en/test_data_en_vi.csv')\nval_df_vi_en = load_and_prepare_data_vi_en('/kaggle/input/data-du-an-cong-nghe-thong-tin/vi-en/validation_data_en_vi.csv')\ntest_df_en_es = load_and_prepare_data_en_es('/kaggle/input/data-du-an-cong-nghe-thong-tin/en_es/test_data_en_es.csv')\nval_df_en_es = load_and_prepare_data_en_es('/kaggle/input/data-du-an-cong-nghe-thong-tin/en_es/validation_data_en_es.csv')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_vi_es = pd.read_csv(\"/kaggle/input/data-test-song-ngu/test_vi_es.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = MT5Tokenizer.from_pretrained('google/mt5-small')","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:07:48.031358Z","iopub.execute_input":"2024-02-27T14:07:48.031871Z","iopub.status.idle":"2024-02-27T14:07:51.330166Z","shell.execute_reply.started":"2024-02-27T14:07:48.031844Z","shell.execute_reply":"2024-02-27T14:07:51.329280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = MT5Config()\n\nconfig.decoder_start_token_id = config.pad_token_id\n\nmodel = MT5ForConditionalGeneration(config)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:08:01.155031Z","iopub.execute_input":"2024-02-27T14:08:01.155726Z","iopub.status.idle":"2024-02-27T14:08:06.629409Z","shell.execute_reply.started":"2024-02-27T14:08:01.155694Z","shell.execute_reply":"2024-02-27T14:08:06.628325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LANG_TOKEN_MAPPING = {\n    'en': '<en>',\n    'vi': '<vi>',\n    'es': '<es>'\n}","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:08:06.631420Z","iopub.execute_input":"2024-02-27T14:08:06.631793Z","iopub.status.idle":"2024-02-27T14:08:06.636419Z","shell.execute_reply.started":"2024-02-27T14:08:06.631758Z","shell.execute_reply":"2024-02-27T14:08:06.635542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\ntokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:08:06.637783Z","iopub.execute_input":"2024-02-27T14:08:06.638340Z","iopub.status.idle":"2024-02-27T14:08:10.819040Z","shell.execute_reply.started":"2024-02-27T14:08:06.638306Z","shell.execute_reply":"2024-02-27T14:08:10.818144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_str(text, tokenizer, seq_len):\n\n    encoded = tokenizer.encode(text, max_length=seq_len, truncation=True, return_tensors=\"pt\")\n\n    if encoded.size(1) < seq_len:\n        padding = torch.full((1, seq_len - encoded.size(1)), tokenizer.pad_token_id)\n        encoded = torch.cat([encoded, padding], dim=1)\n\n    return encoded","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_str_with_lang_tag(text, target_lang, tokenizer, seq_len):\n    text_with_tag = LANG_TOKEN_MAPPING[target_lang] + ' ' + text\n\n    return encode_str(text_with_tag, tokenizer, seq_len)\n\n\ndef process_dataset_with_lang_tag(dataset, input_lang, target_lang, tokenizer, seq_len):\n    dataset['input_encoded'] = dataset[input_lang].apply(lambda x: encode_str_with_lang_tag(x, target_lang, tokenizer, seq_len))\n\n    dataset['target_encoded'] = dataset[target_lang].apply(lambda x: encode_str(x, tokenizer, seq_len))\n\n    return dataset[['input_encoded', 'target_encoded']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = 101\nprocessed_vi_en = process_dataset_with_lang_tag(df_vi_en, 'vi', 'en', tokenizer, seq_len)\nprocessed_en_vi = process_dataset_with_lang_tag(df_vi_en, 'en', 'vi', tokenizer, seq_len)\n\nprocessed_en_es = process_dataset_with_lang_tag(df_en_es, 'en', 'es', tokenizer, seq_len)\nprocessed_es_en = process_dataset_with_lang_tag(df_en_es, 'es', 'en', tokenizer, seq_len)\n\nprocessed_vi_es = process_dataset_with_lang_tag(df_vi_es, 'vi', 'es', tokenizer, seq_len)\nprocessed_es_vi = process_dataset_with_lang_tag(df_vi_es, 'es', 'vi', tokenizer, seq_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_test_vi_en = process_dataset_with_lang_tag(test_df_vi_en, 'vi', 'en', tokenizer, seq_len)\nprocessed_test_en_vi = process_dataset_with_lang_tag(test_df_vi_en, 'en', 'vi', tokenizer, seq_len)\n\nprocessed_test_en_es = process_dataset_with_lang_tag(test_df_en_es, 'en', 'es', tokenizer, seq_len)\nprocessed_test_es_en = process_dataset_with_lang_tag(test_df_en_es, 'es', 'en', tokenizer, seq_len)\n\nprocessed_val_vi_en = process_dataset_with_lang_tag(val_df_vi_en, 'vi', 'en', tokenizer, seq_len)\nprocessed_val_en_vi = process_dataset_with_lang_tag(val_df_vi_en, 'en', 'vi', tokenizer, seq_len)\n\nprocessed_val_en_es = process_dataset_with_lang_tag(test_df_vi_en, 'en', 'es', tokenizer, seq_len)\nprocessed_val_es_en = process_dataset_with_lang_tag(test_df_vi_en, 'es', 'en', tokenizer, seq_len)\n\nprocessed_test_vi_es = process_dataset_with_lang_tag(test_vi_es, 'vi', 'es', tokenizer, seq_len)\nprocessed_test_es_vi = process_dataset_with_lang_tag(test_vi_es, 'es', 'vi', tokenizer, seq_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset_fixed(data, val_count=2000, random_state=42):\n\n    assert len(data) > (test_count + val_count), \n    \n    shuffled_data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n        \n    val_data = shuffled_data[test_count:test_count+val_count]\n    \n    train_data = shuffled_data[test_count+val_count:]\n    \n    return train_data, val_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_vi_en = processed_vi_en.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_data_en_vi = processed_en_vi.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_data_en_es = processed_en_es.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_data_es_en = processed_es_en.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_data_vi_es, val_data_vi_es = split_dataset_fixed(processed_vi_es, val_count=2000, random_state=42)\ntrain_data_es_vi, val_data_es_vi = split_dataset_fixed(processed_es_vi, val_count=2000, random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_data_total = pd.concat([train_data_vi_en, train_data_en_vi, train_data_vi_es, train_data_es_vi, train_data_en_es, train_data_es_en], ignore_index=True)\n\nval_data_total = pd.concat([processed_val_vi_en, processed_val_en_vi, val_data_vi_es, val_data_es_vi, processed_val_en_es, processed_val_es_en], ignore_index=True)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_total.to_csv(\"train_data_total.csv\", index=False)\nval_data_total.to_csv(\"val_data_total.csv\", index=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_total =  pd.read_csv(\"/kaggle/input/data-da-ngu/data_da_ngu/train_data_total.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T11:17:32.143458Z","iopub.execute_input":"2024-02-27T11:17:32.144108Z","iopub.status.idle":"2024-02-27T11:17:44.451047Z","shell.execute_reply.started":"2024-02-27T11:17:32.144073Z","shell.execute_reply":"2024-02-27T11:17:44.449674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data_total =  pd.read_csv(\"/kaggle/input/data-da-ngu/data_da_ngu/val_data_total.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T11:18:13.040825Z","iopub.execute_input":"2024-02-27T11:18:13.041490Z","iopub.status.idle":"2024-02-27T11:18:13.197279Z","shell.execute_reply.started":"2024-02-27T11:18:13.041458Z","shell.execute_reply":"2024-02-27T11:18:13.196316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_total = train_data_total.sample(frac=1, random_state=42).reset_index(drop=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T11:17:44.451960Z","iopub.status.idle":"2024-02-27T11:17:44.452315Z","shell.execute_reply.started":"2024-02-27T11:17:44.452146Z","shell.execute_reply":"2024-02-27T11:17:44.452162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data_total = val_data_total.sample(frac=1, random_state=42).reset_index(drop=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T11:18:15.126554Z","iopub.execute_input":"2024-02-27T11:18:15.127185Z","iopub.status.idle":"2024-02-27T11:18:15.135097Z","shell.execute_reply.started":"2024-02-27T11:18:15.127152Z","shell.execute_reply":"2024-02-27T11:18:15.134270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_string_to_tensor(string):\n    clean_string = re.sub(r'\\s+', ' ', string)  # Loại bỏ khoảng trắng thừa và xuống dòng\n    clean_string = re.sub(r'tensor\\(\\[\\[|\\]\\]\\)', '', clean_string).strip()\n\n    list_of_ints = [int(i) for i in clean_string.split(',')]\n\n    return torch.tensor(list_of_ints)\n\ntrain_data_total['input_encoded'] = train_data_total['input_encoded'].apply(convert_string_to_tensor)\ntrain_data_total['target_encoded'] = train_data_total['target_encoded'].apply(convert_string_to_tensor)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-20T13:04:03.700432Z","iopub.execute_input":"2024-02-20T13:04:03.700718Z","iopub.status.idle":"2024-02-20T13:19:55.303368Z","shell.execute_reply.started":"2024-02-20T13:04:03.700695Z","shell.execute_reply":"2024-02-20T13:19:55.302534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data_total['input_encoded'] = val_data_total['input_encoded'].apply(convert_string_to_tensor)\nval_data_total['target_encoded'] = val_data_total['target_encoded'].apply(convert_string_to_tensor)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        item = self.dataframe.iloc[idx]\n        return {\n            'input': item['input_encoded'],\n            'target': item['target_encoded']\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:37:55.707496Z","iopub.execute_input":"2024-02-27T14:37:55.708325Z","iopub.status.idle":"2024-02-27T14:37:55.713730Z","shell.execute_reply.started":"2024-02-27T14:37:55.708293Z","shell.execute_reply":"2024-02-27T14:37:55.712849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TranslationDataset(train_data_total)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T13:19:58.992745Z","iopub.execute_input":"2024-02-20T13:19:58.993382Z","iopub.status.idle":"2024-02-20T13:19:59.002587Z","shell.execute_reply.started":"2024-02-20T13:19:58.993347Z","shell.execute_reply":"2024-02-20T13:19:59.001736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset = TranslationDataset(val_data_total)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T03:06:04.214534Z","iopub.execute_input":"2024-02-25T03:06:04.214833Z","iopub.status.idle":"2024-02-25T03:06:04.224039Z","shell.execute_reply.started":"2024-02-25T03:06:04.214804Z","shell.execute_reply":"2024-02-25T03:06:04.223211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_vi_es_dataset = TranslationDataset(processed_test_vi_es)\ntest_es_vi_dataset = TranslationDataset(processed_test_es_vi)\n\ntest_vi_es_loader = DataLoader(test_vi_es_dataset, batch_size=15, shuffle=False)\ntest_es_vi_loader = DataLoader(test_es_vi_dataset, batch_size=15, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_vi_en_dataset = TranslationDataset(processed_test_vi_en)\n\ntest_data_es_en_dataset = TranslationDataset(processed_test_es_en)\n\ntest_data_en_vi_dataset = TranslationDataset(processed_test_en_vi)\n\ntest_data_en_es_dataset = TranslationDataset(processed_test_en_es)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_vi_en_test_loader = DataLoader(test_data_vi_en_dataset, batch_size=15, shuffle=False)\n\ntest_data_es_en_test_loader = DataLoader(test_data_es_en_dataset, batch_size=15, shuffle=False)\n\ntest_data_en_vi_test_loader = DataLoader(test_data_en_vi_dataset, batch_size=15, shuffle=False)\n\ntest_data_en_es_test_loader = DataLoader(test_data_en_es_dataset, batch_size=15, shuffle=False)\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset, batch_size=15, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T13:19:59.030815Z","iopub.execute_input":"2024-02-20T13:19:59.031099Z","iopub.status.idle":"2024-02-20T13:19:59.036133Z","shell.execute_reply.started":"2024-02-20T13:19:59.031067Z","shell.execute_reply":"2024-02-20T13:19:59.035276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(val_dataset, batch_size=15, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tensor_to_string(tensor):\n    token_ids = tensor.cpu().numpy()\n    return tokenizer.decode(token_ids, skip_special_tokens=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:44:04.697845Z","iopub.execute_input":"2024-02-27T14:44:04.698683Z","iopub.status.idle":"2024-02-27T14:44:04.703284Z","shell.execute_reply.started":"2024-02-27T14:44:04.698647Z","shell.execute_reply":"2024-02-27T14:44:04.702338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:44:06.027947Z","iopub.execute_input":"2024-02-27T14:44:06.028297Z","iopub.status.idle":"2024-02-27T14:44:06.039211Z","shell.execute_reply.started":"2024-02-27T14:44:06.028268Z","shell.execute_reply":"2024-02-27T14:44:06.038253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_bleu_on_subset_hf(model, test_loader, device, num_batches_to_test, max_length=512):\n    model.eval()\n    metric = load_metric(\"sacrebleu\")\n    references = []\n    hypotheses = []\n\n    with torch.no_grad():\n        for i, batch in enumerate(test_loader):\n            if i >= num_batches_to_test:\n                break \n            input_ids = batch['input'].to(device)\n            target_ids = batch['target'].to(device)\n\n            outputs = model.generate(input_ids, max_length=max_length)\n            hypotheses.extend([tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs])\n            references_batch = [tokenizer.decode(ids, skip_special_tokens=True) for ids in target_ids]\n            references.extend([[ref] for ref in references_batch]) \n\n    for hypothesis, reference in zip(hypotheses, references):\n        metric.add(prediction=hypothesis, reference=reference)\n\n    final_score = metric.compute()[\"score\"]\n    return final_score","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:44:19.913638Z","iopub.execute_input":"2024-02-27T14:44:19.914112Z","iopub.status.idle":"2024-02-27T14:44:19.924042Z","shell.execute_reply.started":"2024-02-27T14:44:19.914071Z","shell.execute_reply":"2024-02-27T14:44:19.923119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = '/kaggle/input/da-ngu-tu-dau-v1-step100000-150000-epoch7/_step_240000_epoch_21.pt'\nmodel_path = '/kaggle/working/'\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:44:19.925352Z","iopub.execute_input":"2024-02-27T14:44:19.925909Z","iopub.status.idle":"2024-02-27T14:44:19.937935Z","shell.execute_reply.started":"2024-02-27T14:44:19.925876Z","shell.execute_reply":"2024-02-27T14:44:19.937112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_save_model(model, train_loader, test_loader, optimizer, device, num_epochs, save_path, tensor_to_string, start_epoch, start_global_step):\n    global_step = 0\n    model.to(device)  \n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        total_loss = 0\n        interval_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n        for batch in progress_bar:\n            if global_step < start_global_step:\n                global_step += 1\n                continue\n            input_ids = batch['input'].to(device)\n            \n            attention_mask = (input_ids != model.config.pad_token_id).long()\n\n            target_ids = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n\n            loss_item = loss.item()\n            total_loss += loss_item\n            interval_loss += loss_item\n            global_step += 1\n\n            if global_step % 5000 == 0:\n                avg_interval_loss = interval_loss / 5000\n                print(f\"Average Loss over last 5000 steps at step {global_step}: {avg_interval_loss}\")\n                interval_loss = 0  \n\n            if global_step % 80000 == 0 :\n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'global_step': global_step,\n                    'epoch': epoch\n                }, f\"{save_path}_step_{global_step}_epoch_{epoch}.pt\")\n\n            progress_bar.set_postfix({'loss': loss_item})\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Trung bình Loss Epoch {epoch+1}: {avg_loss}\")\n\n\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step,\n            'epoch': epoch\n        }, f\"{save_path}_epoch_{epoch}.pt\")\n        \n\n        \n        start_global_step = 0\n        global_step = 0\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_checkpoint(model, optimizer, checkpoint_path, device):\n    model.to(device)\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n    start_global_step = checkpoint['global_step']\n    return start_epoch, start_global_step\n\nmodel.to(device)\n\nstart_epoch, start_global_step = load_checkpoint(model, optimizer, checkpoint_path, device)\n\ntrain_and_save_model(model, train_loader, test_loader, optimizer, device, 30, model_path, tensor_to_string, start_epoch, start_global_step)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T14:44:23.035200Z","iopub.execute_input":"2024-02-27T14:44:23.035885Z","iopub.status.idle":"2024-02-27T14:44:57.575641Z","shell.execute_reply.started":"2024-02-27T14:44:23.035850Z","shell.execute_reply":"2024-02-27T14:44:57.574826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu_score_of_vi_en = test_bleu_on_subset_hf(model, test_data_vi_en_test_loader, device, len(test_data_vi_en_test_loader),  max_length=512)\nprint(f\"BLEU score of vi to en : {bleu_score_of_vi_en}\")\n\nbleu_score_of_vi_es = test_bleu_on_subset_hf(model, test_data_vi_es_test_loader, device, len(test_data_vi_en_test_loader),  max_length=512)\nprint(f\"BLEU score of vi to es : {bleu_score_of_vi_es}\")\n\nbleu_score_of_es_en = test_bleu_on_subset_hf(model, test_data_es_en_test_loader, device, len(test_data_vi_en_test_loader),  max_length=512)\nprint(f\"BLEU score of es to en : {bleu_score_of_es_en}\")\n\nbleu_score_of_en_vi = test_bleu_on_subset_hf(model, test_data_en_vi_test_loader, device, len(test_data_vi_en_test_loader),  max_length=512)\nprint(f\"BLEU score of en to vi : {bleu_score_of_en_vi}\")\n\nbleu_score_of_en_es = test_bleu_on_subset_hf(model, test_data_en_es_test_loader, device, len(test_data_vi_en_test_loader),  max_length=512)\nprint(f\"BLEU score of en to es : {bleu_score_of_en_es}\")\n\nbleu_score_of_es_vi = test_bleu_on_subset_hf(model, test_data_es_vi_test_loader, device, len(test_data_vi_en_test_loader),  max_length=512)\nprint(f\"BLEU score of es to vi : {bleu_score_of_es_vi}\")","metadata":{},"execution_count":null,"outputs":[]}]}